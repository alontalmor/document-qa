{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:40.585195Z",
     "start_time": "2018-09-21T05:29:37.738121Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 140)\n",
    "pd.set_option('display.width', 2000)\n",
    "import sys,os\n",
    "from IPython.core.debugger import Tracer\n",
    "from IPython.core.debugger import BdbQuit_excepthook\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import nltk\n",
    "import io\n",
    "import json\n",
    "import time\n",
    "from pandas import ExcelWriter\n",
    "from ast import literal_eval\n",
    "import hashlib\n",
    "import unicodedata\n",
    "import subprocess\n",
    "import datetime\n",
    "m = hashlib.md5()\n",
    "from nltk.metrics import *\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import zipfile\n",
    "import pymongo\n",
    "import re\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:40.670952Z",
     "start_time": "2018-09-21T05:29:40.588046Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "EXP_NAME = 'MSMARCO_Googled_Sample'\n",
    "EVAL_SET = 'dev' # train, dev , which split to run on \n",
    "WRITE_EVIDENCE = 0 # should we write the evidence to file\n",
    "SEARCHRESULTS_FROM_FILES = 1\n",
    "FILES_PER_QUESTION = 20 # how many evidence files to build per question\n",
    "LIMIT_TRAIN_SIZE = 100\n",
    "PRODUCE_ONLY_SAMPLE = 0\n",
    "RUN_EXPERIMENTS = 0\n",
    "\n",
    "# DataSets\n",
    "USE_MSMARCO = 1\n",
    "USE_SEARCHQA = 0\n",
    "USE_TRIVIAQA = 0\n",
    "USE_SQUAD = 0\n",
    "USE_COMPWEBQ = 0\n",
    "\n",
    "# Dataset Specific \n",
    "# Squad\n",
    "USE_SQUAD_ORG_CONTEXT = 0\n",
    "# ComplexWebQuestions\n",
    "ADD_SPLITS_TO_TRAIN = 1\n",
    "\n",
    "# GENERAL\n",
    "#PRODUCE_ONLY_SAMPLE = True\n",
    "EVIDENCE_DIR = '/Users/alontalmor/Documents/dev/datasets/triviaqa/triviaqa-rc/'\n",
    "\n",
    "# Tests\n",
    "CALC_ANSWER_IN_GOOGLE_PERC = 0\n",
    "GOOGLE_FILTERED_FILES = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:29:54.834220Z",
     "start_time": "2018-05-15T07:29:54.713840Z"
    }
   },
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:40.683182Z",
     "start_time": "2018-09-21T05:29:40.675680Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "Data = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SearchQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:40.695379Z",
     "start_time": "2018-09-21T05:29:40.686673Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = '/Users/alontalmor/Documents/dev/datasets/SearchQA/data_json/000488-5243_jeopardy_beginningend_600.json'\n",
    "with open(filename,'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:40.718211Z",
     "start_time": "2018-09-21T05:29:40.698454Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From\" this to this is an idiom meaning from the start of a meal (or something else) to the end    98\n",
       "Name: question, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data)['question'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:40.727509Z",
     "start_time": "2018-09-21T05:29:40.720959Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"From\" this to this is an idiom meaning from the start of a meal (or something else) to the end'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:40.796470Z",
     "start_time": "2018-09-21T05:29:40.732521Z"
    }
   },
   "outputs": [],
   "source": [
    "if USE_SEARCHQA:\n",
    "    def load_searchqa(dirname,filename):\n",
    "        print(os.path.join(dirname,filename +'.zip'))\n",
    "        with zipfile.ZipFile(os.path.join(dirname,filename +'.zip'),'r') as myzip:\n",
    "            with myzip.open(filename) as myfile:\n",
    "                searchqa_json = json.load(myfile)\n",
    "        #with gzip.open(os.path.join(dirname,filename +'.zip'),'rb') as myzip:\n",
    "        #    searchqa_json=pd.DataFrame(json.loads(myzip.read()))\n",
    "\n",
    "        return searchqa_json\n",
    "    if USE_SEARCHQA and EVAL_SET == 'dev':\n",
    "         Data['searchqa'] = load_searchqa('/Users/alontalmor/Documents/dev/datasets/SearchQA/data_json/','val')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSMARCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.212874Z",
     "start_time": "2018-09-21T05:29:40.799497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alontalmor/Documents/dev/datasets/MSMARCO/dev_v2.1.json.gz\n",
      "101093\n",
      "9350\n"
     ]
    }
   ],
   "source": [
    "if USE_MSMARCO:\n",
    "    import gzip\n",
    "    def load_msmarco(dirname,filename):\n",
    "        print(os.path.join(dirname,filename +'.gz'))\n",
    "        with gzip.open(os.path.join(dirname,filename +'.gz'),'rb') as myzip:\n",
    "            msmarco = pd.DataFrame(json.loads(myzip.read()))\n",
    "\n",
    "        print(len(msmarco))\n",
    "        msmarco = msmarco[msmarco['answers'].str[0] != 'No Answer Present.']\n",
    "        msmarco = msmarco[msmarco['answers'].str[0] != 'Yes']\n",
    "        msmarco = msmarco[msmarco['answers'].str[0] != 'No']\n",
    "        msmarco = msmarco[msmarco['answers'].str[0] != '']\n",
    "\n",
    "        \n",
    "\n",
    "        # Filter very long answers:\n",
    "        msmarco = msmarco[msmarco['answers'].str[0].apply(len) < 30]\n",
    "        \n",
    "        # filtering if no well formed answers? \n",
    "        # len(Data['msmarco'][Data['msmarco']['wellFormedAnswers'] != '[]'])\n",
    "        \n",
    "        print(len(msmarco))\n",
    "\n",
    "        return msmarco\n",
    "\n",
    "    if USE_MSMARCO and EVAL_SET == 'dev':\n",
    "         Data['MSMARCO'] = load_msmarco('/Users/alontalmor/Documents/dev/datasets/MSMARCO/','dev_v2.1.json')\n",
    "    \n",
    "    # fileter no answer persent, yes and no answers:\n",
    "    \n",
    "    #Data['msmarco']['answers'].str[0].value_counts()[0:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.270798Z",
     "start_time": "2018-09-21T05:29:49.215333Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def load_squad(dirname,filename):\n",
    "    with zipfile.ZipFile(os.path.join(dirname,filename +'.zip'),'r') as myzip:\n",
    "        with myzip.open(filename) as myfile:\n",
    "            squad_json = json.load(myfile)\n",
    "\n",
    "    squad_questions = []\n",
    "    contexts = []\n",
    "    for part in squad_json['data']:\n",
    "        for para in part['paragraphs']:\n",
    "            contexts.append(para['context'])\n",
    "            for qas in para['qas']:\n",
    "                question = qas.copy()\n",
    "                question['title'] = part['title']\n",
    "                question['context_id'] = len(contexts) - 1\n",
    "                squad_questions.append(question)\n",
    "    squad_questions = pd.DataFrame(squad_questions)\n",
    "\n",
    "    # removing version 2 samples! \n",
    "    squad_questionsV1 = squad_questions[~squad_questions['is_impossible']].copy(deep=True)\n",
    "    del squad_questionsV1['is_impossible']\n",
    "    del squad_questionsV1['plausible_answers']\n",
    "\n",
    "    # empty entity pages\n",
    "    squad_questionsV1['EntityPages'] = np.empty((len(squad_questionsV1), 0)).tolist()\n",
    "\n",
    "    # empty search results\n",
    "    squad_questionsV1['SearchResults'] = np.empty((len(squad_questionsV1), 0)).tolist()\n",
    "\n",
    "    return squad_questionsV1, contexts\n",
    "    \n",
    "if USE_SQUAD and EVAL_SET == 'dev':\n",
    "     Data['Squad'], org_contexts = load_squad('../data/Squad/','dev-v2.0.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.290524Z",
     "start_time": "2018-09-21T05:29:49.273302Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if USE_TRIVIAQA and EVAL_SET == 'dev':\n",
    "    with zipfile.ZipFile('../data/TriviaQA/unfiltered-web-dev.json.zip','r') as myzip:\n",
    "        with myzip.open('unfiltered-web-dev.json') as myfile:\n",
    "            questions = json.load(myfile)\n",
    "            Data['TriviaQA'] = pd.DataFrame(questions['Data'])\n",
    "            \n",
    "    # empty entity pages\n",
    "    Data['TriviaQA']['EntityPages'] = np.empty((len(Data['TriviaQA']), 0)).tolist()\n",
    "\n",
    "    # empty search results\n",
    "    Data['TriviaQA']['SearchResults']  = Data['TriviaQA']['SearchResults'].astype(object)\n",
    "    Data['TriviaQA']['SearchResults'] = np.empty((len(Data['TriviaQA']), 0)).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComplexWebQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.367724Z",
     "start_time": "2018-09-21T05:29:49.292954Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# comparing web triavia-rc dev number of question to unfiltered number of questions\n",
    "if USE_COMPWEBQ and EVAL_SET == 'dev':\n",
    "    rl_input_df = pd.DataFrame()\n",
    "    #data_dir = '/Users/alontalmor/Dropbox/Apps/WebKB/webkb_dev_data/RL_preproc_data/rl_cascade1_epoch0-8/'\n",
    "    data_dir = '../data/V2_dev_splitpoints/'\n",
    "    filename = 'dev.json.zip'\n",
    "    if filename.find('.json.zip') > -1:\n",
    "        print(filename)\n",
    "        with zipfile.ZipFile(data_dir + filename, 'r') as myzip:\n",
    "            with myzip.open(filename.replace('.zip', '')) as myfile:\n",
    "                curr_batch = pd.DataFrame(json.load(myfile))\n",
    "        curr_batch = curr_batch[(curr_batch[['split_part1', 'split_part2']].isnull(\n",
    "        ) * 1.0).sum(axis=1) == 0]  # removing null values\n",
    "        curr_batch['traj_id'] = curr_batch['ID'] + curr_batch['comp'] + curr_batch['split_part1'].str.replace(\" \",\"\") \\\n",
    "                                + ',' + curr_batch['split_part2'].str.replace(\" \",\"\")\n",
    "        if len(rl_input_df) > 0:\n",
    "            len_before_filter = len(curr_batch)\n",
    "            curr_batch = curr_batch[\n",
    "                ~curr_batch['traj_id'].isin(rl_input_df['traj_id'])]\n",
    "        curr_batch['filename'] = filename\n",
    "        rl_input_df = rl_input_df.append(curr_batch, ignore_index=True)\n",
    "\n",
    "    rl_input_df = rl_input_df.set_index('traj_id')\n",
    "\n",
    "    # dropping exact duplicate splits\n",
    "    print('size before drop dups: ' + str(len(rl_input_df)))\n",
    "    rl_input_df = rl_input_df.drop_duplicates(\n",
    "        ['ID', 'comp', 'split_part1', 'split_part2'])\n",
    "    print('size after drop dups: ' + str(len(rl_input_df)))\n",
    "\n",
    "    dataset_filename = '../../../mturk/compqgen/final/complexwebquestions_V1_1/ComplexWebQuestions_dev'\n",
    "    with open(dataset_filename + '.json', 'r') as outfile:\n",
    "        complexwebquestions = pd.DataFrame(json.load(outfile))\n",
    "\n",
    "    rl_input_df = rl_input_df.merge(\n",
    "        pd.DataFrame(complexwebquestions)[['answers', 'ID']], on='ID', how='inner')\n",
    "    rl_input_df.rename(columns={'answers_y': 'answers'}, inplace=True)\n",
    "    del rl_input_df['answers_x']\n",
    "    Data['ComplexWebQuestions'] = rl_input_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSMARCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.373916Z",
     "start_time": "2018-09-21T05:29:49.370507Z"
    }
   },
   "outputs": [],
   "source": [
    "if USE_MSMARCO and EVAL_SET == 'train':\n",
    "    Data['MSMARCO'] = load_msmarco('/Users/alontalmor/Documents/dev/datasets/MSMARCO/','train_v2.1.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.379789Z",
     "start_time": "2018-09-21T05:29:49.376318Z"
    }
   },
   "outputs": [],
   "source": [
    "if USE_SQUAD and EVAL_SET == 'train':\n",
    "    Data['Squad'], org_contexts = load_squad('../data/Squad/','train-v2.0.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.398198Z",
     "start_time": "2018-09-21T05:29:49.381957Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if USE_TRIVIAQA and EVAL_SET == 'train':\n",
    "    with zipfile.ZipFile('../data/TriviaQA/unfiltered-web-train.json.zip','r') as myzip:\n",
    "        with myzip.open('unfiltered-web-train.json') as myfile:\n",
    "            questions = json.load(myfile)\n",
    "            triviaqa_dataset = pd.DataFrame(questions['Data'])\n",
    "            \n",
    "    # empty entity pages\n",
    "    triviaqa_dataset['EntityPages'] = np.empty((len(triviaqa_dataset), 0)).tolist()\n",
    "\n",
    "    # empty search results \n",
    "    triviaqa_dataset['SearchResults']  = triviaqa_dataset['SearchResults'].astype(object)\n",
    "    triviaqa_dataset['SearchResults'] = np.empty((len(triviaqa_dataset), 0)).tolist()\n",
    "    \n",
    "    Data['TriviaQA'] = triviaqa_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComplexWebQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.567663Z",
     "start_time": "2018-09-21T05:29:49.401054Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if USE_COMPWEBQ and EVAL_SET == 'train':\n",
    "    rl_input_df = pd.DataFrame()\n",
    "    #data_dir = '/Users/alontalmor/Dropbox/Apps/WebKB/webkb_dev_data/RL_train_data/rl_cascade1_epoch0-8/'\n",
    "    # V2 Code\n",
    "    data_dir = '../data/V2_train_splitpoints/'\n",
    "    for dirname, dirnames, filenames in os.walk(data_dir):\n",
    "\n",
    "        # making sure noisy sup is added first (because of the default MIN_REWARD_TRESH values\n",
    "        #if 'noisy_sup_rewarded.json.zip' in filenames:\n",
    "        #    filenames.remove('noisy_sup_rewarded.json.zip')\n",
    "        #    filenames = ['noisy_sup_rewarded.json.zip'] + filenames\n",
    "        # V2 code\n",
    "        if 'train.json.zip' in filenames:\n",
    "            filenames.remove('train.json.zip')\n",
    "            filenames = ['train.json.zip'] + filenames\n",
    "\n",
    "        for filename in filenames[0:2]:\n",
    "\n",
    "            if filename.find('.json.zip')>-1:\n",
    "                print(filename)\n",
    "                with zipfile.ZipFile(data_dir + filename,'r') as myzip:\n",
    "                    with myzip.open(filename.replace('.zip','')) as myfile:\n",
    "                        curr_batch = pd.DataFrame(json.load(myfile))\n",
    "                curr_batch = curr_batch[(curr_batch[['split_part1', 'split_part2']].isnull() * 1.0).sum(axis=1) == 0] # removing null values\n",
    "                # V2 Code\n",
    "                if len(rl_input_df) > 0:\n",
    "                    curr_batch = curr_batch[curr_batch['ID'].isin(rl_input_df['ID'])]\n",
    "\n",
    "                curr_batch['traj_id'] = curr_batch['ID'] + curr_batch['comp'] + curr_batch['split_part1'].str.replace(\" \",\"\") \\\n",
    "                                            + ',' + curr_batch['split_part2'].str.replace(\" \",\"\")\n",
    "                if len(rl_input_df)>0:\n",
    "                    len_before_filter = len(curr_batch)\n",
    "                    curr_batch = curr_batch[~curr_batch['traj_id'].isin(rl_input_df['traj_id'])]\n",
    "                curr_batch['filename'] = filename\n",
    "                rl_input_df = rl_input_df.append(curr_batch, ignore_index=True)\n",
    "\n",
    "    dataset_filename = '../../../mturk/compqgen/final/complexwebquestions_V1_1/ComplexWebQuestions_train'\n",
    "    with open(dataset_filename + '.json', 'r') as outfile:\n",
    "        complexwebquestions = pd.DataFrame(json.load(outfile))\n",
    "\n",
    "    rl_input_df = rl_input_df.merge(\n",
    "        pd.DataFrame(complexwebquestions)[['answers', 'ID']], on='ID', how='inner')\n",
    "    rl_input_df.rename(columns={'answers_y': 'answers'}, inplace=True)\n",
    "    del rl_input_df['answers_x']        \n",
    "\n",
    "    rl_input_df = rl_input_df.set_index('traj_id')\n",
    "\n",
    "    # dropping exact duplicate splits\n",
    "    print('size before drop dups: ' + str(len(rl_input_df)))\n",
    "    rl_input_df = rl_input_df.drop_duplicates(['ID', 'comp', 'split_part1', 'split_part2'])\n",
    "    print('size after drop dups: ' + str(len(rl_input_df)))\n",
    "    \n",
    "    # checking overlab between V1 training question and V2\n",
    "    #dataset_filename = '../../mturk/compqgen/final/complexwebquestions/ComplexWebQuestions_train'\n",
    "    #with open(dataset_filename + '.json', 'r') as outfile:\n",
    "    #    complexwebquestions_org = pd.DataFrame(json.load(outfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.577598Z",
     "start_time": "2018-09-21T05:29:49.570187Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if USE_COMPWEBQ:\n",
    "    dataset_filename = '../data/ComplexWebQuestions_' + EVAL_SET\n",
    "    with open(dataset_filename + '.json', 'r') as outfile:\n",
    "        complexwebquestions_org = pd.DataFrame(json.load(outfile)).set_index('ID')\n",
    "\n",
    "    rl_input_df.loc[rl_input_df['comp'] == 'composition','composition_answer'] = \\\n",
    "        list(complexwebquestions_org.loc[rl_input_df[rl_input_df['comp'] == 'composition']['ID'],'composition_answer'].astype(object))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.623985Z",
     "start_time": "2018-09-21T05:29:49.580120Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if USE_COMPWEBQ:\n",
    "    questions = []   \n",
    "    for ind in tqdm(range(len(rl_input_df)), total=len(rl_input_df), ncols=80, desc=\"scoring\"):\n",
    "        question = rl_input_df.iloc[ind]\n",
    "        if question['comp'] == 'conjunction':\n",
    "            q_copied = question.copy()\n",
    "            questions.append(q_copied)\n",
    "            if ADD_SPLITS_TO_TRAIN:\n",
    "                q_copied = question.copy()\n",
    "                q_copied['question'] = question['split_part1']\n",
    "                questions.append(q_copied)\n",
    "                q_copied = question.copy()\n",
    "                q_copied['question'] = question['split_part2']\n",
    "                questions.append(q_copied)\n",
    "        elif question['comp'] == 'composition':\n",
    "            q_copied = question.copy()\n",
    "            q_copied['ID']\n",
    "            questions.append(q_copied)\n",
    "            if ADD_SPLITS_TO_TRAIN:\n",
    "                q_copied = question.copy()\n",
    "                q_copied['question'] = question['split_part1']\n",
    "                q_copied['answers'] = [{'aliases':[],'answer':question['composition_answer']}]\n",
    "                questions.append(q_copied)\n",
    "                q_copied = question.copy()\n",
    "                q_copied['question'] = question['split_part2']\n",
    "                q_copied['question'] = q_copied['question'].replace('%composition',question['composition_answer'])\n",
    "                questions.append(q_copied)\n",
    "        else:\n",
    "            q_copied = question.copy()\n",
    "            questions.append(q_copied)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.632619Z",
     "start_time": "2018-09-21T05:29:49.625941Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if USE_COMPWEBQ:\n",
    "    questions = pd.DataFrame(questions)\n",
    "    # dropping exact duplicate splits\n",
    "    print('size before drop dups: ' + str(len(questions)))\n",
    "    questions = questions.drop_duplicates(['question'])\n",
    "    print('size after drop dups: ' + str(len(questions)))\n",
    "    questions = questions.reset_index(drop=True)\n",
    "    Data['ComplexWebQuestions'] = questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ComplexWebQuestions to TriviaQA format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert to triviaqa format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dev and train are converted in the same place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:49.636749Z",
     "start_time": "2018-09-21T05:29:49.634323Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "Data_triviaqa_format = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSMARCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:52.795008Z",
     "start_time": "2018-09-21T05:29:49.638669Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scoring: 100%|████████████████████████████| 9350/9350 [00:03<00:00, 3030.51it/s]\n"
     ]
    }
   ],
   "source": [
    "if USE_MSMARCO:\n",
    "    questions = Data['MSMARCO']\n",
    "    questions_triviaqa_format = pd.DataFrame()\n",
    "    questions_triviaqa_format['QuestionId'] = questions['query_id']\n",
    "    questions_triviaqa_format['Question'] = questions['query']\n",
    "    questions_triviaqa_format['SearchResults'] = [[] for x in range(len(questions_triviaqa_format))]\n",
    "    questions_triviaqa_format['EntityPages'] = [[] for x in range(len(questions_triviaqa_format))]\n",
    "    questions_triviaqa_format['QuestionSource'] = ''\n",
    "    all_answers = []\n",
    "    for ind,q in tqdm(questions.iterrows(),total=len(questions), ncols=80, desc=\"scoring\"):\n",
    "        filtered_answer = []\n",
    "        for answer in q['answers']:\n",
    "            if len(answer) > 0:\n",
    "                filtered_answer.append(answer)\n",
    "        \n",
    "        \n",
    "        triviaqa_formated_answers = {'Aliases':[],'NormalizedAliases':[], \\\n",
    "                                    'NormalizedValue':'', \\\n",
    "                                     'Type':'FreeForm','Value':''}\n",
    "        triviaqa_formated_answers['Value'] = filtered_answer[0]\n",
    "        triviaqa_formated_answers['NormalizedValue'] = ' '.join(word_tokenize(filtered_answer[0].lower()))\n",
    "        aliases = [answer for answer in filtered_answer]\n",
    "        aliases = list(set(aliases))\n",
    "        triviaqa_formated_answers['Aliases'] = aliases\n",
    "        \n",
    "        triviaqa_formated_answers['NormalizedAliases'] = \\\n",
    "            [' '.join(word_tokenize(word.lower())) for word in triviaqa_formated_answers['Aliases']]\n",
    "        all_answers.append(triviaqa_formated_answers)\n",
    "\n",
    "    questions_triviaqa_format['Answer'] = all_answers\n",
    "    questions_triviaqa_format = questions_triviaqa_format[questions_triviaqa_format['Answer'].notnull()] \n",
    "    \n",
    "    Data_triviaqa_format['MSMARCO'] = questions_triviaqa_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:52.894875Z",
     "start_time": "2018-09-21T05:29:52.797288Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if USE_SQUAD:\n",
    "    questions = Data['Squad']\n",
    "    if PRODUCE_ONLY_SAMPLE:\n",
    "        questions = questions[0:100]\n",
    "        \n",
    "    questions_triviaqa_format = pd.DataFrame()\n",
    "    questions_triviaqa_format['QuestionId'] = questions['id']\n",
    "    questions_triviaqa_format['Question'] = questions['question']\n",
    "    if USE_SQUAD_ORG_CONTEXT:\n",
    "        all_contexts = []\n",
    "        for ind,q in tqdm(questions.iterrows(),total=len(questions), ncols=80, desc=\"org context\"): \n",
    "            all_contexts.append([{'title':'','snippet':org_contexts[q['context_id']]}])\n",
    "        questions_triviaqa_format['SearchResults'] = all_contexts\n",
    "    else:\n",
    "        questions_triviaqa_format['SearchResults'] = [[] for x in range(len(questions_triviaqa_format))]\n",
    "    questions_triviaqa_format['EntityPages'] = [[] for x in range(len(questions_triviaqa_format))]\n",
    "    questions_triviaqa_format['QuestionSource'] = ''\n",
    "    all_answers = []\n",
    "    for ind,q in tqdm(questions.iterrows(),total=len(questions), ncols=80, desc=\"appending answers\"):\n",
    "        filtered_answer = []\n",
    "        for answer in q['answers']:\n",
    "            if len(answer['text']) > 0:\n",
    "                filtered_answer.append(answer)\n",
    "        \n",
    "        \n",
    "        triviaqa_formated_answers = {'Aliases':[],'NormalizedAliases':[], \\\n",
    "                                    'NormalizedValue':'', \\\n",
    "                                     'Type':'FreeForm','Value':''}\n",
    "        triviaqa_formated_answers['Value'] = filtered_answer[0]['text']\n",
    "        triviaqa_formated_answers['NormalizedValue'] = ' '.join(word_tokenize(filtered_answer[0]['text'].lower()))\n",
    "        aliases = [answer['text'] for answer in filtered_answer]\n",
    "        aliases = list(set(aliases))\n",
    "        triviaqa_formated_answers['Aliases'] = aliases\n",
    "        \n",
    "        triviaqa_formated_answers['NormalizedAliases'] = \\\n",
    "            [' '.join(word_tokenize(word.lower())) for word in triviaqa_formated_answers['Aliases']]\n",
    "        all_answers.append(triviaqa_formated_answers)\n",
    "\n",
    "    questions_triviaqa_format['Answer'] = all_answers\n",
    "    questions_triviaqa_format = questions_triviaqa_format[questions_triviaqa_format['Answer'].notnull()] \n",
    "    \n",
    "    Data_triviaqa_format['Squad'] = questions_triviaqa_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ComplexWebQuestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:52.972297Z",
     "start_time": "2018-09-21T05:29:52.897096Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if USE_COMPWEBQ:\n",
    "    questions = Data['ComplexWebQuestions']\n",
    "    if PRODUCE_ONLY_SAMPLE:\n",
    "        questions = questions[0:100]\n",
    "    questions_triviaqa_format = pd.DataFrame()\n",
    "    questions_triviaqa_format['QuestionId'] = questions['ID'] + '_' + questions.index.astype(str)\n",
    "    questions_triviaqa_format['Question'] = questions['question']\n",
    "    questions_triviaqa_format['SearchResults'] = [[] for x in range(len(questions_triviaqa_format))]\n",
    "    questions_triviaqa_format['EntityPages'] = [[] for x in range(len(questions_triviaqa_format))]\n",
    "    questions_triviaqa_format['QuestionSource'] = ''\n",
    "    all_answers = []\n",
    "    for ind,q in tqdm(questions.iterrows(),total=len(questions), ncols=80, desc=\"scoring\"):\n",
    "        if q['answers'][0]['answer'] == None or q['answers'][0]['answer'] == '':\n",
    "            all_answers.append(None)\n",
    "        else:\n",
    "            filtered_answer = []\n",
    "            for answer in q['answers']:\n",
    "                if len(answer['answer']) > 0:\n",
    "                    filtered_answer.append(answer)\n",
    "\n",
    "            triviaqa_formated_answers = {'Aliases':[],'NormalizedAliases':[], \\\n",
    "                                        'NormalizedValue':'', \\\n",
    "                                         'Type':'FreeForm','Value':''}\n",
    "            triviaqa_formated_answers['Value'] = filtered_answer[0]['answer']\n",
    "            triviaqa_formated_answers['NormalizedValue'] = ' '.join(word_tokenize(filtered_answer[0]['answer'].lower()))\n",
    "            for answer in filtered_answer:\n",
    "                triviaqa_formated_answers['Aliases'] += answer['aliases']\n",
    "                triviaqa_formated_answers['Aliases'].append(answer['answer'])\n",
    "\n",
    "            triviaqa_formated_answers['NormalizedAliases'] = \\\n",
    "                [' '.join(word_tokenize(word.lower())) for word in triviaqa_formated_answers['Aliases']]\n",
    "            all_answers.append(triviaqa_formated_answers)\n",
    "\n",
    "    questions_triviaqa_format['Answer'] = all_answers\n",
    "    questions_triviaqa_format = questions_triviaqa_format[questions_triviaqa_format['Answer'].notnull()]  \n",
    "    Data_triviaqa_format['ComplexWebQuestions'] = questions_triviaqa_format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TriviaQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:52.980259Z",
     "start_time": "2018-09-21T05:29:52.975468Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if USE_TRIVIAQA:\n",
    "    if PRODUCE_ONLY_SAMPLE:\n",
    "        Data['TriviaQA'] = Data['TriviaQA'][0:100]\n",
    "    Data_triviaqa_format['TriviaQA'] = Data['TriviaQA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appending snippets to the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google snippets are appended to all sub-datasets in the same manner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From local mongo (currently only for ComplexWebQuestions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:52.988243Z",
     "start_time": "2018-09-21T05:29:52.982980Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if USE_COMPWEBQ and WRITE_EVIDENCE:\n",
    "    MONGODB_URI = 'mongodb://127.0.0.1:27017/webkb'\n",
    "    mongo_client = pymongo.MongoClient(MONGODB_URI)\n",
    "    db = mongo_client.get_default_database()\n",
    "    SearchCache = db['SearchResults_Cache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.023140Z",
     "start_time": "2018-09-21T05:29:52.991323Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if USE_COMPWEBQ:\n",
    "    if WRITE_EVIDENCE:\n",
    "        found = 0\n",
    "        questions_triviaqa_format = Data_triviaqa_format['ComplexWebQuestions']\n",
    "        questions_triviaqa_format = questions_triviaqa_format.set_index('QuestionId')\n",
    "        questions_triviaqa_format['SearchResults'] = None\n",
    "        questions_triviaqa_format['SearchResults'] = questions_triviaqa_format['SearchResults'].astype(object)\n",
    "        for QuestionId,question in tqdm(questions_triviaqa_format.iterrows(), total=len(questions_triviaqa_format), ncols=80,\\\n",
    "                        desc=\"appending google search results\"):\n",
    "            CacheResults = SearchCache.find(\n",
    "                {'querystr': question['Question'], \"page\": 0, \"type\": 'SCREEN'})\n",
    "            CacheResults_Count = CacheResults.count()\n",
    "            if CacheResults_Count>0:\n",
    "                found += 1\n",
    "                cahched_item = CacheResults.next()\n",
    "                questions_triviaqa_format.at[QuestionId,'SearchResults'] = cahched_item['results']\n",
    "        questions_triviaqa_format = questions_triviaqa_format.reset_index()\n",
    "\n",
    "        Data_triviaqa_format['ComplexWebQuestions_Googled'] = questions_triviaqa_format\n",
    "        del Data_triviaqa_format['ComplexWebQuestions']\n",
    "    else:\n",
    "        Data_triviaqa_format['ComplexWebQuestions_Googled'] = Data_triviaqa_format.pop('ComplexWebQuestions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.066240Z",
     "start_time": "2018-09-21T05:29:53.025706Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def append_google_from_files(questions_triviaqa_format,googled_dir):\n",
    "    question_count = 0\n",
    "\n",
    "    questions_triviaqa_format = questions_triviaqa_format.set_index('QuestionId')\n",
    "    questions_triviaqa_format['SearchResults'] = None\n",
    "    questions_triviaqa_format['SearchResults'] = questions_triviaqa_format['SearchResults'].astype(object)\n",
    "\n",
    "    for dirname, dirnames, filenames in os.walk(googled_dir):\n",
    "        for filename in tqdm(filenames, total=len(filenames), ncols=80, desc='iterating over all googled files'):\n",
    "            with zipfile.ZipFile(googled_dir + '/' + filename,'r') as myzip:\n",
    "                with myzip.open(filename.replace('.zip','')) as myfile:\n",
    "                    googled = json.load(myfile)\n",
    "\n",
    "            for googled_question in googled:\n",
    "                question_count += 1\n",
    "                if 'QuestionId' in googled_question:\n",
    "                    questions_triviaqa_format.at[googled_question['QuestionId'],\\\n",
    "                                             'SearchResults'] = googled_question['google_results']\n",
    "                else:\n",
    "                    questions_triviaqa_format.at[googled_question['id'],\\\n",
    "                                             'SearchResults'] = googled_question['google_results']\n",
    "\n",
    "    questions_triviaqa_format = questions_triviaqa_format.reset_index()\n",
    "    print('number of questions googled')\n",
    "    print(question_count)\n",
    "    \n",
    "    return questions_triviaqa_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.078005Z",
     "start_time": "2018-09-21T05:29:53.068840Z"
    },
    "code_folding": [
     0,
     1
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# TriviaQA\n",
    "if USE_TRIVIAQA and SEARCHRESULTS_FROM_FILES:\n",
    "    if WRITE_EVIDENCE:\n",
    "        if EVAL_SET == 'dev':\n",
    "            Data_triviaqa_format['TriviaQA_Googled'] = append_google_from_files(Data_triviaqa_format['TriviaQA'], \\\n",
    "                                                                       '../data/triviaqa_googled_dev')\n",
    "        else:\n",
    "            Data_triviaqa_format['TriviaQA_Googled'] = append_google_from_files(Data_triviaqa_format['TriviaQA'], \\\n",
    "                                                                   '../data/triviaqa_googled_train')\n",
    "        del Data_triviaqa_format['TriviaQA']\n",
    "    else:\n",
    "        Data_triviaqa_format['TriviaQA_Googled'] = Data_triviaqa_format.pop('TriviaQA')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.089125Z",
     "start_time": "2018-09-21T05:29:53.079995Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# MSMARCO\n",
    "if USE_MSMARCO and SEARCHRESULTS_FROM_FILES:\n",
    "    if WRITE_EVIDENCE:\n",
    "        if EVAL_SET == 'dev':\n",
    "            Data_triviaqa_format['MSMARCO_Googled'] = append_google_from_files(Data_triviaqa_format['MSMARCO'], \\\n",
    "                                                                       '/Users/alontalmor/Dropbox/Apps/WebKB/MSMARCO/dev')\n",
    "        else:\n",
    "            Data_triviaqa_format['MSMARCO_Googled'] = append_google_from_files(Data_triviaqa_format['MSMARCO'], \\\n",
    "                                                                   '/Users/alontalmor/Dropbox/Apps/WebKB/MSMARCO/train')\n",
    "        del Data_triviaqa_format['MSMARCO']\n",
    "    else:\n",
    "        Data_triviaqa_format['MSMARCO_Googled'] = Data_triviaqa_format.pop('MSMARCO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.104085Z",
     "start_time": "2018-09-21T05:29:53.091674Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# Squad \n",
    "if USE_SQUAD and USE_SQUAD_ORG_CONTEXT:\n",
    "    Data_triviaqa_format['Squad_Org'] = Data_triviaqa_format.pop('Squad')\n",
    "elif USE_SQUAD and SEARCHRESULTS_FROM_FILES:\n",
    "    if WRITE_EVIDENCE:\n",
    "        if EVAL_SET == 'dev':\n",
    "            Data_triviaqa_format['Squad_Googled'] = append_google_from_files(Data_triviaqa_format['Squad'], \\\n",
    "                                                                       '../data/Squad/dev')\n",
    "        else:\n",
    "            Data_triviaqa_format['Squad_Googled'] = append_google_from_files(Data_triviaqa_format['Squad'], \\\n",
    "                                                                   '../data/Squad/train')  \n",
    "        del Data_triviaqa_format['Squad']\n",
    "    else:\n",
    "        Data_triviaqa_format['Squad_Googled'] = Data_triviaqa_format.pop('Squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.138574Z",
     "start_time": "2018-09-21T05:29:53.106721Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "MSMARCO_Googled\n",
      "print how many question we found google results for\n",
      "0\n",
      "final question count\n",
      "9350\n"
     ]
    }
   ],
   "source": [
    "# Post PostProcessing\n",
    "for dataset in Data_triviaqa_format.keys():\n",
    "    print('---------------------------------')\n",
    "    print(dataset)\n",
    "    Data_triviaqa_format[dataset] = \\\n",
    "        Data_triviaqa_format[dataset][Data_triviaqa_format[dataset]['SearchResults'].notnull()]\n",
    "        \n",
    "    print('print how many question we found google results for')\n",
    "    print((Data_triviaqa_format[dataset]['SearchResults'].apply(len)>0).sum())\n",
    "    \n",
    "\n",
    "    Data_triviaqa_format[dataset] = \\\n",
    "        Data_triviaqa_format[dataset][Data_triviaqa_format[dataset]['SearchResults'].notnull()]\n",
    "\n",
    "    Data_triviaqa_format[dataset] = \\\n",
    "        Data_triviaqa_format[dataset][Data_triviaqa_format[dataset]['Answer'].notnull()]\n",
    "\n",
    "    print('final question count')\n",
    "    print(len(Data_triviaqa_format[dataset]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-07-12T08:52:27.539131Z",
     "start_time": "2018-07-12T08:52:27.536603Z"
    }
   },
   "source": [
    "# Combine all datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation also supports sampling (to reduce size or change number of examples from each dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.163812Z",
     "start_time": "2018-09-21T05:29:53.140969Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# combining all the training sets\n",
    "questions_triviaqa_format = pd.DataFrame()\n",
    "for dataset in Data_triviaqa_format.keys():\n",
    "    Data_triviaqa_format[dataset]['dataset'] = dataset\n",
    "    questions_triviaqa_format = \\\n",
    "        questions_triviaqa_format.append(Data_triviaqa_format[dataset],ignore_index=True)\n",
    "if EVAL_SET == 'train' and LIMIT_TRAIN_SIZE != -1:\n",
    "    if len(questions_triviaqa_format) >= LIMIT_TRAIN_SIZE:\n",
    "        questions_triviaqa_format = questions_triviaqa_format.sample(n=LIMIT_TRAIN_SIZE)\n",
    "        \n",
    "if EVAL_SET == 'dev':\n",
    "    # We don't need more than 8000 in the mixed dev set.\n",
    "    if len(questions_triviaqa_format) >= 8000:\n",
    "        questions_triviaqa_format = questions_triviaqa_format.sample(n=8000)\n",
    "    \n",
    "del Data_triviaqa_format\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.172526Z",
     "start_time": "2018-09-21T05:29:53.165933Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    100\n",
       "Name: SearchResults, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_triviaqa_format['SearchResults'].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-31T15:54:10.417162Z",
     "start_time": "2018-05-31T15:54:10.414441Z"
    }
   },
   "source": [
    "# Building Evidence Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.178470Z",
     "start_time": "2018-09-21T05:29:53.175148Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "#print('checking if there exist cases in which there are no answers?')\n",
    "#print(len(questions_triviaqa_format[questions_triviaqa_format['Answer'].agg(lambda x: x['Value']).apply(len)==0]['Answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.198639Z",
     "start_time": "2018-09-21T05:29:53.191592Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "triviaqa_dict = {}\n",
    "triviaqa_dict['Data'] = questions_triviaqa_format\n",
    "triviaqa_dict['Domain'] = 'unfiltered-web'\n",
    "triviaqa_dict['Split'] = EVAL_SET\n",
    "triviaqa_dict['VerifiedEval'] = False\n",
    "triviaqa_dict['Version'] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.209355Z",
     "start_time": "2018-09-21T05:29:53.200957Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "data_df = triviaqa_dict['Data']\n",
    "data_df = data_df.set_index('QuestionId')\n",
    "org_data_df = data_df.copy(deep=True)\n",
    "data_df = data_df.rename(columns = {'SearchResults':'OrgSearchResults'})\n",
    "data_df['SearchResults'] = None\n",
    "data_df = data_df[data_df['OrgSearchResults'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build googled evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.316027Z",
     "start_time": "2018-09-21T05:29:53.211678Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# OLD create a query to file map\n",
    "if False and not USE_SQUAD_ORG_CONTEXT:\n",
    "    if WRITE_EVIDENCE and not os.path.isdir(EVIDENCE_DIR + 'evidence/'):\n",
    "        os.mkdir(EVIDENCE_DIR + 'evidence/')\n",
    "\n",
    "    if WRITE_EVIDENCE and not os.path.isdir(EVIDENCE_DIR +'evidence/' + EXP_NAME):\n",
    "        os.mkdir(EVIDENCE_DIR + 'evidence/' + EXP_NAME)\n",
    "\n",
    "    train_file_ind = int(0)\n",
    "    for i in tqdm(range(len(data_df)), total=len(data_df), ncols=80, desc=\"building evidence files\"):\n",
    "        question = data_df.iloc[i]\n",
    "        questionID = data_df.index[i]\n",
    "        # building 10 text files out of 100 snippets\n",
    "        SearchResults = []\n",
    "        files = []\n",
    "        filenames = []\n",
    "        file_ind = 0\n",
    "        google_results = question['OrgSearchResults']\n",
    "        train_file_ind += 1\n",
    "\n",
    "\n",
    "        if WRITE_EVIDENCE and not os.path.isdir(EVIDENCE_DIR + 'evidence/' + EXP_NAME + '/' + str(int(train_file_ind / 100))):\n",
    "            os.mkdir(EVIDENCE_DIR + 'evidence/' + EXP_NAME + '/' + str(int(train_file_ind / 100)))\n",
    "            #if train_file_ind % 1000 == 0:\n",
    "            #    print(EVIDENCE_DIR + 'evidence/' + EXP_NAME + '/' + str(int(train_file_ind / 100)))\n",
    "\n",
    "        # go over all google snippets (usually 100)\n",
    "        for ind, g in enumerate(google_results):\n",
    "            file_ind = file_ind % FILES_PER_QUESTION\n",
    "            if len(files) <= file_ind:\n",
    "                file_name = EXP_NAME + '/' + str(int(train_file_ind / 100)) + \"/\" + \\\n",
    "                    questionID + '_' + str(file_ind) + '.txt'\n",
    "                #SearchResults.append({'Rank':ind, 'Description':g['snippet'],'Title':g['title'],'DisplayUrl':g['url'] , \\\n",
    "                #              'Url':g['url'] + file_name.replace('/','_').replace('.txt',''),'Filename':file_name })\n",
    "                # Moving to empty evidence text ( it will be taken from the files)\n",
    "                SearchResults.append({'Rank':ind, 'Description':'','Title':'','DisplayUrl':'' , \\\n",
    "                              'Url':g['url'] + file_name.replace('/','_').replace('.txt',''),'Filename':file_name })\n",
    "                \n",
    "                files.append('')\n",
    "                filenames.append(file_name)\n",
    "\n",
    "            files[file_ind] += str(\n",
    "                ind) + '. ' + g['title'] + '\\n' + g['snippet'] + '\\n'\n",
    "\n",
    "            file_ind += 1\n",
    "\n",
    "        # saving files\n",
    "        if WRITE_EVIDENCE:\n",
    "            for file_str, file_name in zip(files, filenames):\n",
    "                with open(EVIDENCE_DIR + 'evidence/' + file_name, 'w') as outfile:\n",
    "                    outfile.write(file_str)\n",
    "\n",
    "        data_df.at[questionID, 'SearchResults'] = SearchResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.327466Z",
     "start_time": "2018-09-21T05:29:53.319049Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# check folder name distribution (200 folders per dataset)\n",
    "if False:\n",
    "    x = []\n",
    "    for ind in list(data_df.index):\n",
    "        m = hashlib.md5()\n",
    "        m.update(ind.encode())\n",
    "        questionID_hex = m.hexdigest()\n",
    "        x.append(sum(questionID_hex.encode()) % 200)\n",
    "    pd.Series(x).hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.578905Z",
     "start_time": "2018-09-21T05:29:53.329992Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# write_evidence\n",
    "def write_evidence(data_df,EVIDENCE_DIR):\n",
    "    if WRITE_EVIDENCE and not os.path.isdir(EVIDENCE_DIR + 'multiqa_evidence/'):\n",
    "        os.mkdir(EVIDENCE_DIR + 'multiqa_evidence/')\n",
    "\n",
    "    all_search_results = []\n",
    "    train_file_ind = int(0)\n",
    "    for i in tqdm(range(len(data_df)), total=len(data_df), ncols=80, desc=\"building evidence files\"):\n",
    "    #for i in tqdm(range(1), total=1, ncols=80, desc=\"building evidence files\"):\n",
    "        question = data_df.iloc[i]\n",
    "        questionID = data_df.index[i]\n",
    "        # building 10 text files out of 100 snippets\n",
    "        SearchResults = []\n",
    "        files = []\n",
    "        filenames = []\n",
    "        file_ind = 0\n",
    "        google_results = question['OrgSearchResults']\n",
    "        train_file_ind += 1\n",
    "\n",
    "        # creating a unique question identifier\n",
    "        m = hashlib.md5()\n",
    "        if type(questionID) != str:\n",
    "            questionID = str(questionID)\n",
    "        m.update(questionID.encode())\n",
    "        questionID_hex = m.hexdigest()\n",
    "        folder_ind = str(sum(questionID_hex.encode()) % 200)\n",
    "\n",
    "        if WRITE_EVIDENCE and not os.path.isdir(EVIDENCE_DIR + 'multiqa_evidence/' + question['dataset'] ):\n",
    "            os.mkdir(EVIDENCE_DIR + 'multiqa_evidence/' + question['dataset'] )\n",
    "\n",
    "        if WRITE_EVIDENCE and not os.path.isdir(EVIDENCE_DIR + 'multiqa_evidence/' + question['dataset'] + '/' + folder_ind):\n",
    "            os.mkdir(EVIDENCE_DIR + 'multiqa_evidence/' + question['dataset'] + '/' + folder_ind)\n",
    "            #if train_file_ind % 1000 == 0:\n",
    "            #    print(EVIDENCE_DIR + 'evidence/' + EXP_NAME + '/' + str(int(train_file_ind / 100)))\n",
    "\n",
    "        # go over all google snippets (usually 100)\n",
    "        if WRITE_EVIDENCE:\n",
    "            for ind, g in enumerate(google_results):\n",
    "                file_ind = file_ind % FILES_PER_QUESTION\n",
    "                if len(files) <= file_ind:\n",
    "                    file_name = question['dataset'] + '/' + folder_ind + \"/\" + \\\n",
    "                        questionID + '_' + str(file_ind) + '.txt'\n",
    "                    # Moving to empty evidence text ( it will be taken from the files)\n",
    "                    SearchResults.append({'Rank':ind, 'Description':'','Title':'','DisplayUrl':'' , \\\n",
    "                                  'Url':question['dataset'] + '_' + file_name.replace('/','_').replace('.txt',''), \\\n",
    "                                          'Filename':file_name })\n",
    "\n",
    "                    files.append('')\n",
    "                    filenames.append(file_name)\n",
    "\n",
    "                if WRITE_EVIDENCE:\n",
    "                    if len(google_results)>1:\n",
    "                        files[file_ind] += str(\n",
    "                            ind) + '. ' + g['title'] + '\\n' + g['snippet'] + '\\n'\n",
    "                    else:\n",
    "                        # this coveres cases with only one search result, or original context like squad\n",
    "                        if len(g['title'])>0:\n",
    "                            files[file_ind] += g['title'] + '\\n' + g['snippet'] + '\\n'\n",
    "                        else:\n",
    "                            files[file_ind] += g['snippet'] \n",
    "\n",
    "                file_ind += 1\n",
    "        else:\n",
    "            for file_ind in range(FILES_PER_QUESTION):\n",
    "                file_name = question['dataset'] + '/' + folder_ind + \"/\" + \\\n",
    "                    questionID + '_' + str(file_ind) + '.txt'\n",
    "                if os.path.exists(EVIDENCE_DIR + 'multiqa_evidence/' + file_name):\n",
    "                    # Moving to empty evidence text ( it will be taken from the files)\n",
    "                    SearchResults.append({'Rank':file_ind, 'Description':'','Title':'','DisplayUrl':'' , \\\n",
    "                                  'Url':question['dataset'] + '_' + file_name.replace('/','_').replace('.txt',''), \\\n",
    "                                          'Filename':file_name })\n",
    "                else:\n",
    "                    #print(EVIDENCE_DIR + 'multiqa_evidence/' + file_name)\n",
    "                    break\n",
    "\n",
    "        # saving files\n",
    "        if WRITE_EVIDENCE:\n",
    "            for file_str, file_name in zip(files, filenames):\n",
    "                with open(EVIDENCE_DIR + 'multiqa_evidence/' + file_name, 'w') as outfile:\n",
    "                    outfile.write(file_str)\n",
    "\n",
    "        all_search_results.append(SearchResults)            \n",
    "    \n",
    "    data_df['SearchResults'] = all_search_results\n",
    "        \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.894992Z",
     "start_time": "2018-09-21T05:29:53.580682Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "building evidence files: 100%|███████████████| 100/100 [00:00<00:00, 324.67it/s]\n"
     ]
    }
   ],
   "source": [
    "#%lprun -f write_evidence write_evidence(data_df,EVIDENCE_DIR)\n",
    "data_df = write_evidence(data_df,EVIDENCE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.903410Z",
     "start_time": "2018-09-21T05:29:53.897622Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_df[data_df['SearchResults'].isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T07:33:42.310094Z",
     "start_time": "2018-05-15T07:33:42.279265Z"
    }
   },
   "source": [
    "# Calc answer in google percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.973797Z",
     "start_time": "2018-09-21T05:29:53.905886Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "INSPECT_EVIDENCE = False\n",
    "if CALC_ANSWER_IN_GOOGLE_PERC:\n",
    "    answer_found_mat = np.zeros((len(data_df),100))\n",
    "    q_ind = 0\n",
    "    for ind,question in tqdm(data_df.iterrows(),total=len(data_df), ncols=80, \\\n",
    "                             desc='checking how many gold answer are within question snippets'):\n",
    "        normalized_aliases = question['Answer']['NormalizedAliases']\n",
    "\n",
    "        for s_ind,result in enumerate(question['SearchResults']):\n",
    "            for alias in normalized_aliases:\n",
    "                \n",
    "                with open(EVIDENCE_DIR + 'multiqa_evidence/' + result['Filename'], 'r') as outfile:\n",
    "                    result_text = outfile.read()\n",
    "                #result_text = result['Title'] + ' ' + result['Description']\n",
    "                p = re.compile(r'\\b({0})\\b'.format(re.escape(alias)), re.IGNORECASE)\n",
    "                res = re.findall(p, result_text)\n",
    "                #if result_text.find(alias)>-1:\n",
    "                 #   answer_found_mat[q_ind , s_ind] = 1\n",
    "                if len(res) > 0:\n",
    "                    if INSPECT_EVIDENCE:\n",
    "                        print('-------------')\n",
    "                        print('Question  ' + question['Question'])\n",
    "                        print('Answer:  ' + alias)\n",
    "                        print('Context:')\n",
    "                        print(result_text) \n",
    "                    answer_found_mat[q_ind , s_ind] = 1\n",
    "                    break\n",
    "        q_ind += 1\n",
    "        \n",
    "        if INSPECT_EVIDENCE and q_ind > 10:\n",
    "            break\n",
    "                #else:\n",
    "                #    print(alias + ' --- ' + result_text)\n",
    "    \n",
    "    print('answer are within question snippets {0}%'.format(100.0* (answer_found_mat.sum(axis=1)>0).sum() / len(data_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sanity Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.985581Z",
     "start_time": "2018-09-21T05:29:53.976439Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Question            0\n",
       "OrgSearchResults    0\n",
       "EntityPages         0\n",
       "QuestionSource      0\n",
       "Answer              0\n",
       "dataset             0\n",
       "SearchResults       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# additional sanity tests\n",
    "(data_df.isnull()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:53.996377Z",
     "start_time": "2018-09-21T05:29:53.988246Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# additional sanity tests\n",
    "if data_df['SearchResults'].isnull().sum()==0:\n",
    "    data_df['SearchResults'].apply(len).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:54.010639Z",
     "start_time": "2018-09-21T05:29:53.999954Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.54545454545455"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df['SearchResults'].apply(len) == 0]['Question'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:54.020608Z",
     "start_time": "2018-09-21T05:29:54.013848Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.06741573033708"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df[data_df['SearchResults'].apply(len) > 0]['Question'].apply(len).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:54.044586Z",
     "start_time": "2018-09-21T05:29:54.025024Z"
    },
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "if WRITE_EVIDENCE:\n",
    "    found = 0\n",
    "    not_found = 0\n",
    "    for i in tqdm(range(len(data_df)), total=len(data_df), ncols=80, desc=\"checking if filenames exist\"):\n",
    "        question = data_df.iloc[i]\n",
    "        questionID = data_df.index[i]\n",
    "        for ind, g in enumerate(question['SearchResults']):\n",
    "            if os.path.exists(EVIDENCE_DIR + 'multiqa_evidence/' + g['Filename']):\n",
    "                found += 1\n",
    "            else:\n",
    "                not_found += 1\n",
    "\n",
    "    print('found : {0}'.format(found))\n",
    "    print('not_found : {0}'.format(not_found))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:54.055823Z",
     "start_time": "2018-09-21T05:29:54.047229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of examples saved: 100\n"
     ]
    }
   ],
   "source": [
    "del data_df['OrgSearchResults']\n",
    "\n",
    "if EVAL_SET == 'train':\n",
    "    data_df = data_df[data_df['SearchResults'].apply(len)>0]\n",
    "\n",
    "# randomizing the samples:\n",
    "data_df = data_df.sample(frac=1)\n",
    "\n",
    "print('Amount of examples saved: %d' % len(data_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:54.116989Z",
     "start_time": "2018-09-21T05:29:54.058477Z"
    },
    "code_folding": [
     3
    ],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if not WRITE_EVIDENCE:\n",
    "    if not os.path.isdir('../output/' ):\n",
    "        os.mkdir('../output/' )\n",
    "    if not os.path.isdir('../output/' + EXP_NAME):\n",
    "        os.mkdir('../output/' + EXP_NAME)\n",
    "\n",
    "    if EVAL_SET == 'dev':\n",
    "        triviaqa_dict['Data'] = data_df.reset_index().to_dict(orient='rows')\n",
    "        with zipfile.ZipFile('../output/' + EXP_NAME + '/' + \"unfiltered-web-dev.json.zip\", \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n",
    "            zip_file.writestr('unfiltered-web-dev.json', json.dumps(triviaqa_dict, sort_keys=True, indent=4))\n",
    "    else:\n",
    "        triviaqa_dict['Data'] = data_df.reset_index().to_dict(orient='rows')\n",
    "        with zipfile.ZipFile('../output/' + EXP_NAME + '/' + \"unfiltered-web-train.json.zip\", \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n",
    "            zip_file.writestr('unfiltered-web-train.json', json.dumps(triviaqa_dict, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing two different batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T11:31:50.657742Z",
     "start_time": "2018-09-21T11:31:50.652991Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '/Users/alontalmor/Documents/dev/datasets/NewsQA/newsqa-data-v1/cnn/stories/13012604e3203c18df09289dfedd14cde67cf40b.story'\n",
    "with open(path,'r') as f:\n",
    "    x = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T11:32:17.949817Z",
     "start_time": "2018-09-21T11:32:17.945985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' were frightened by a series of killings and threats by Muslim extremists ordering them to convert to '"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.replace('\\n',' ')[660:762]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T11:30:17.289365Z",
     "start_time": "2018-09-21T11:30:17.285461Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Johannesburg (CNN) -- Miffed by a visa delay that led the Dalai Lama to cancel a trip to South Africa, Archbishop Desmond Tutu lashed out at his government Tuesday, saying it had acted worse than apartheid regimes and had forgotten all that the nation stood for.\\n\\n\"When we used to apply for passports under the apartheid government, we never knew until the last moment what their decision was,\" Tutu said at a news conference. \"Our government is worse than the apartheid government because at least you were expecting it from the apartheid government.\\n\\n\"I have to say that I can\\'t believe this. I really can\\'t believe this,\" Tutu said. \"You have to wake me up and tell me this is actually happening here.\"\\n\\nThe Dalai Lama scrapped his planned trip to South Africa this week after the nation failed to issue him a visa in time, his spokesman said.\\n\\nVisa applications for him and his entourage were submitted to the South African High Commission in New Delhi, India, at the end of August, and original passports were submitted on September 20, more than two weeks ago, a statement on his website said.\\n\\nHowever, South Africa\\'s foreign affairs office said it did not refuse a visa.\\n\\n\"South Africa will not comment on the decision, because it is not our decision, it is his decision,\" according to spokesman Clayson Monyela, who said the visa application was still under consideration.\\n\\nThe Dalai Lama had been invited to the country to receive the Mahatma Gandhi International Award for Peace and Reconciliation and to speak at a number of events, including a lecture in honor of Tutu\\'s 80th birthday. Tutu and the Dalai Lama are recipients of the Nobel Peace Prize.\\n\\nTutu said he would pray for the defeat of South Africa\\'s government, led by the African National Congress (ANC), which is rooted in the fight against the system of apartheid, or legal racial separation, that was present in South Africa until 1994.\\n\\n\"You are disgraceful,\" Tutu said about the government. \"You are behaving in a way that is totally at variance with the things for which we stood.\"\\n\\nThe ANC plans to call on government officials to explain to South Africans why the visa process was delayed, spokesman Jackson Mtembu said. He said everyone was in the dark about this matter.\\n\\nBut he also suggested that Tutu calm down. A comparison to apartheid regimes, he said, was unfair.\\n\\nThis is not the first time the Dalai Lama has not been able to visit South Africa. In 2009, South Africa refused the Tibetan spiritual leader a visa to attend an international peace conference, saying it was not in the country\\'s interest for him to attend.\\n\\nIn refusing the 2009 application, South Africa said that if the Dalai Lama attended the conference, the focus would shift away from the 2010 World Cup, the global soccer championship it was hosting.\\n\\n\"We cannot allow focus to shift to China and Tibet,\" presidential spokesman Thabo Masebe said, adding that South Africa had gained much from its trading relationship with China.\\n\\nThe Dalai Lama fled Tibet in 1959 after a failed uprising against Chinese rule, and China pressures governments around the world to deny him any legitimacy.\\n\\nSpeculation surfaced Tuesday that this year\\'s visit was also affected by South Africa\\'s relationship with China.\\n\\nSouth African Vice President Kgalema Motlanthe visited Beijing last week and met with Chinese President Hu Jintao to discuss bolstering bilateral ties.\\n\\nMotlanthe said South Africa was ready to boost the strategic partnership between the two countries to a new stage, according to the official Chinese news agency Xinhua.\\n\\nBut Monyela said the application had nothing to do with China.\\n\\n\"We are a sovereign nation which takes decisions in our domestic interest,\" Monyela said.\\n\\nThe Dalai Lama posted a message on Twitter last week that said: \"Even if the Chinese leave nothing but ashes, Tibet will rise from these ashes as a free country even if it takes a long time to do so.\"\\n\\nKim Norgaard, CNN\\'s Johannesburg bureau chief, contributed to this report.\\n\\n@highlight\\n\\nNEW: The African National Congress says Desmond Tutu should calm down\\n\\n@highlight\\n\\nTutu says the government is acting worse than apartheid regime\\n\\n@highlight\\n\\nThe Dalai Lama says South Africa did not issue a visa on time\\n\\n@highlight\\n\\nHe was denied a visa to South Africa in 2009'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:54.127424Z",
     "start_time": "2018-09-21T05:29:54.119143Z"
    }
   },
   "outputs": [],
   "source": [
    "if RUN_EXPERIMENTS:\n",
    "    with zipfile.ZipFile('../output/compwebq_with_triviaqa/unfiltered-web-train.json.zip','r') as myzip:\n",
    "            with myzip.open('unfiltered-web-train.json') as myfile:\n",
    "                questions = json.load(myfile)\n",
    "                compwebq_with_triviaqa = pd.DataFrame(questions['Data'])\n",
    "    compwebq1 = compwebq_with_triviaqa[compwebq_with_triviaqa['QuestionId'].str.startswith('WebQ')]\n",
    "    del compwebq_with_triviaqa\n",
    "    len(compwebq1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:54.140176Z",
     "start_time": "2018-09-21T05:29:54.130066Z"
    }
   },
   "outputs": [],
   "source": [
    "#RUN_EXPERIMENTS = 1\n",
    "if RUN_EXPERIMENTS:\n",
    "    with zipfile.ZipFile('../output/ComplexWebQuestions/unfiltered-web-dev.json.zip','r') as myzip:\n",
    "        with myzip.open('unfiltered-web-dev.json') as myfile:\n",
    "            questions = json.load(myfile)\n",
    "            compwebq_triviaqa_full_dev = pd.DataFrame(questions['Data'])\n",
    "    compwebq2 = compwebq_triviaqa_full_dev[compwebq_triviaqa_full_dev['QuestionId'].str.startswith('WebQ')]\n",
    "    del compwebq_triviaqa_full_dev\n",
    "    print(len(compwebq2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:54.147572Z",
     "start_time": "2018-09-21T05:29:54.142595Z"
    }
   },
   "outputs": [],
   "source": [
    "if RUN_EXPERIMENTS:\n",
    "    x = compwebq2.merge(compwebq1,on='QuestionId',how='inner')\n",
    "    ((x['SearchResults_x'].apply(len) - x['SearchResults_y'].apply(len))!=0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Google "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:54.205374Z",
     "start_time": "2018-09-21T05:29:54.150026Z"
    },
    "code_folding": [
     0
    ],
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#GOOGLE_FILTERED_FILES = True\n",
    "if GOOGLE_FILTERED_FILES:\n",
    "    dataset = 'SearchQA'\n",
    "    BATCH_SIZE = 200\n",
    "    \n",
    "    question_for_google = Data_triviaqa_format[dataset].to_dict(orient='rows')\n",
    "\n",
    "    import dropbox\n",
    "    dbx = dropbox.Dropbox('7j6m2s1jYC0AAAAAAAHy69fu0OxDAU3fPbIjjarqr_1zalj8Mvypf8U71BoLT-AD')\n",
    "\n",
    "    print('iterating over files')\n",
    "    offset = 0\n",
    "    while True:\n",
    "        curr_batch_webanswer_question = question_for_google[offset:offset+BATCH_SIZE]\n",
    "        \n",
    "        if len(curr_batch_webanswer_question) == 0:\n",
    "            break\n",
    "        \n",
    "        new_questions = []\n",
    "        for question in curr_batch_webanswer_question: \n",
    "            goog_question = {}\n",
    "            goog_question['goog_query'] = question['Question']\n",
    "            goog_question['QuestionId'] = question['QuestionId']\n",
    "            new_questions.append(goog_question)\n",
    "        \n",
    "        for_goog_dict = {'target_dir': dataset + '/' + EVAL_SET ,'questions': new_questions}\n",
    "\n",
    "        filename = dataset + '-' + EVAL_SET + '-' + str(offset) + '_for_goog.json'\n",
    "        with zipfile.ZipFile(filename + '.zip', \"w\", zipfile.ZIP_DEFLATED) as zip_file:\n",
    "            zip_file.writestr(filename, json.dumps(for_goog_dict, sort_keys=True, indent=4))\n",
    "        \n",
    "        print(filename)\n",
    "        \n",
    "        with open(filename + '.zip', \"rb\") as f:\n",
    "            dbx.files_upload(f.read(), '/google/' + filename + '.zip', mode = dropbox.files.WriteMode.overwrite)\n",
    "\n",
    "        os.remove(filename + '.zip')    \n",
    "        offset += BATCH_SIZE\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-22T01:25:36.024511Z",
     "start_time": "2018-09-22T01:25:36.002507Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iterating over all googled files: 100%|███████| 23/23 [00:00<00:00, 3217.24it/s]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    DIR = '/Users/alontalmor/Dropbox/Apps/WebKB/google/'\n",
    "    for dirname, dirnames, filenames in os.walk(DIR):\n",
    "        for filename in tqdm(filenames, total=len(filenames), ncols=80, desc='iterating over all googled files'):\n",
    "            if filename.find('processing')>-1:\n",
    "                os.rename(DIR + filename, DIR + filename.replace('processing','for_goog'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google SearchQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:54.218551Z",
     "start_time": "2018-09-21T05:29:54.208298Z"
    }
   },
   "outputs": [],
   "source": [
    "if GOOGLE_FILTERED_FILES:\n",
    "    DIR = '/Users/alontalmor/Documents/dev/datasets/SearchQA/data_json/'\n",
    "    question_for_google = []\n",
    "    for dirname, dirnames, filenames in os.walk(DIR):\n",
    "        for filename in tqdm(filenames, total=len(filenames), ncols=80, desc='iterating over all googled files'):\n",
    "            with open(DIR + filename,'r') as f:\n",
    "                try:\n",
    "                    single_question_dict = json.load(f)\n",
    "                    question_for_google.append({'Question':single_question_dict['question'], \\\n",
    "                                                'QuestionId':single_question_dict['id']})\n",
    "                except:\n",
    "                    print('bad json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-09-21T05:29:54.225170Z",
     "start_time": "2018-09-21T05:29:54.220855Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "if GOOGLE_FILTERED_FILES:\n",
    "    Data_triviaqa_format = {}\n",
    "    dataset = 'SearchQA'\n",
    "    Data_triviaqa_format[dataset] = pd.DataFrame(question_for_google)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "927px",
    "left": "0px",
    "right": "1587.01px",
    "top": "109px",
    "width": "325px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
